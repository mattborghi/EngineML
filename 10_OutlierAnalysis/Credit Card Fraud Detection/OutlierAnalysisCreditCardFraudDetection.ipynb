{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "cf3e997c-1878-4bc7-be79-1bbb96e31a9d",
        "_uuid": "e32c6893aff5e81e29671bb63836a78d4c1f4f2c"
      },
      "cell_type": "markdown",
      "source": "Hi all,\n\nThis is my first Kernel on Kaggle.\n\nI will try to build a rough model using Gaussian Distribution to detect Anamolous transactions.\n\n**Reason behind using Gaussian Distribution:-**  <br>\nIf I can summarize what Andrew Ng has mentioned in his lecture on Anomaly detection is \nSupervised Classification technique is not the perfect candidate for highly imbalanced data. In this case it is \n 0.172% (near to 0)\n\nIf We think from the persepctive of building the model to find out the anomalous data which is not seen very frequently \nWe should go for Anomaly detection technique using Gaussian Distribution.  \n"
    },
    {
      "metadata": {
        "_cell_guid": "8ba10cf2-fea4-49fb-a5d0-80412d3cbcae",
        "_uuid": "bae1f5ea58db298919959facea375d58c379b043",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom numpy import genfromtxt\nfrom scipy.stats import multivariate_normal\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score , average_precision_score\nfrom sklearn.metrics import precision_score, precision_recall_curve\n%matplotlib inline\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bfa0c373-17b9-48a1-b8f1-4fcc831b20fd",
        "_uuid": "be9fdf7a34980b73d70488b23c8b766fa14e9f80"
      },
      "cell_type": "markdown",
      "source": "I will be defining the below two functions which are required to calculate Gaussian Distribution of the normalized variables provided in the dataset (V1, V2 ....V28, Amount ).  <br>\nnote- These functions will be invoked for building the model\n\n1) Find out mu and Sigma for the dataframe variables passed to this function. <br>\n      ----\n2) Calculate Probability Distribution for the each row (I will explain why we need Probality for each row as we proceed) <br>\n       ----\n       \nFormula:- \nif each example x has N dimensiona(features) then below formula is used to calculate the P value <br>\n**P(x) = p(x1,u1,sigma1^2)p(x2,u2,sigma2^2)p(x3,u3,sigma3^2).....p(xn,un,sigma'N'^2)**\n      ---"
    },
    {
      "metadata": {
        "_cell_guid": "f9b5bb38-710d-4166-803e-8a4564516540",
        "collapsed": true,
        "_uuid": "78b33b829a9cd3745b6340026bf197d8a0d1f3c4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def estimateGaussian(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.cov(dataset.T)\n    return mu, sigma\n\ndef multivariateGaussian(dataset,mu,sigma):\n    p = multivariate_normal(mean=mu, cov=sigma)\n    return p.pdf(dataset)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "376a04fb-9285-48cc-920f-4fe71966a878",
        "_uuid": "0d7e27320f5b6982a05efdb552c927a282781e53"
      },
      "cell_type": "markdown",
      "source": "Below is the most crucial function used to detect how well we are doing with our subset (Cross validation subset) .\nI have decided values for Epsilon for detecting the fradulent transactions from the Subsets.  <br><br>\n**(Tip :- Ideally you should provide range of epsilon values, due to time constraint on running this kernel i have provided few values here for demonstration purpose)**\n\n **For now remember Epsilon value is the threshold value below which we will mark transaction as Anomalous.**\n           ----\n\nRewriting above sentense again \nP(x) for X if less than the epsilon value then mark that transaction as anomalous transaction. \n\nWe need to maintain healthy balance between the Recall and Precision . We may get Recall value above 0.80 and close to 0.90 here but at the expense of reducing our precision which is not advisable.\n"
    },
    {
      "metadata": {
        "_cell_guid": "09caa539-d1f2-499d-b00b-5532e6002f14",
        "collapsed": true,
        "_uuid": "6aaf6c9af7c49177bd4da13d85d082ab0e57984b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def selectThresholdByCV(probs,gt):\n    best_epsilon = 0\n    best_f1 = 0\n    f = 0\n    farray = []\n    Recallarray = []\n    Precisionarray = []\n    epsilons = (0.0000e+00, 1.0527717316e-70, 1.0527717316e-50, 1.0527717316e-24)\n    #epsilons = np.asarray(epsilons)\n    for epsilon in epsilons:\n        predictions = (p_cv < epsilon)\n        f = f1_score(train_cv_y, predictions, average = \"binary\")\n        Recall = recall_score(train_cv_y, predictions, average = \"binary\")\n        Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n        farray.append(f)\n        Recallarray.append(Recall)\n        Precisionarray.append(Precision)\n        print ('For below Epsilon')\n        print(epsilon)\n        print ('F1 score , Recall and Precision are as below')\n        print ('Best F1 Score %f' %f)\n        print ('Best Recall Score %f' %Recall)\n        print ('Best Precision Score %f' %Precision)\n        print ('-'*40)\n        if f > best_f1:\n            best_f1 = f\n            best_recall = Recall\n            best_precision = Precision\n            best_epsilon = epsilon    \n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.7, 0.3])\n    #plt.subplot(3,1,1)\n    plt.plot(farray ,\"ro\")\n    plt.plot(farray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('F1 score vs Epsilon value')\n    ax.annotate('Best F1 Score', xy=(best_epsilon,best_f1), xytext=(best_epsilon,best_f1))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"F1 Score\") \n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n    #plt.subplot(3,1,2)\n    plt.plot(Recallarray ,\"ro\")\n    plt.plot(Recallarray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('Recall vs Epsilon value')\n    ax.annotate('Best Recall Score', xy=(best_epsilon,best_recall), xytext=(best_epsilon,best_recall))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"Recall Score\") \n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n    #plt.subplot(3,1,3)\n    plt.plot(Precisionarray ,\"ro\")\n    plt.plot(Precisionarray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('Precision vs Epsilon value')\n    ax.annotate('Best Precision Score', xy=(best_epsilon,best_precision), xytext=(best_epsilon,best_precision))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"Precision Score\") \n    plt.show()\n    return best_f1, best_epsilon",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f765fc12-530b-4829-a82d-852323f945f6",
        "_uuid": "f897b22bc44b2f1650a2830a2ea01764357ce4db"
      },
      "cell_type": "markdown",
      "source": "Lets Read the dataset \n        ---"
    },
    {
      "metadata": {
        "_cell_guid": "e0707e73-37f4-422d-a961-71a51b0dd42e",
        "collapsed": true,
        "_uuid": "a74a641e3836e1bd8c5b5cd4ecf8fcacf14928f6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input//creditcard.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1706fc65-8907-44e4-92b4-9c8519168e19",
        "_uuid": "a249d0c35eb6bfdca826b356636576995d265cc8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(train_df.columns.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7c9b9fcc-fbbf-4601-b530-898d57d32882",
        "_uuid": "6708840763cd7cffb17632c91f59d38ef0d59b01"
      },
      "cell_type": "markdown",
      "source": "**Lets visualize which features are not much of help in detecting the anamoly **"
    },
    {
      "metadata": {
        "_cell_guid": "1e157f13-827b-47bd-bd3b-14ab15e77366",
        "_uuid": "2a1e5d225e6bc9e7296462de4525b671e5e4cb6d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "v_features = train_df.iloc[:,1:29].columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "be34dc83-a4e0-4757-9a9d-54e6d0bb67e9",
        "_uuid": "587eb7aae36154d75461e79b7c131eed8c8976a6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,8*4))\ngs = gridspec.GridSpec(7, 4)\nfor i, cn in enumerate(train_df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(train_df[cn][train_df.Class == 1], bins=50)\n    sns.distplot(train_df[cn][train_df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "233b3258-df03-4ff9-a770-56b618fcaf9c",
        "_uuid": "fa7c9cce52d535265aed42cb15a362ccb2709b7a"
      },
      "cell_type": "markdown",
      "source": "**Feature Importance**\n    -------\nLets use Feqture importqnce to get rid of unwanted features whose existance will not improve our prediction model. <br>\nI have used random forest classifier to identify the influential fetures. You can validate the below result with the feature analysis I conducted above."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0cb31da65f030dfe94cbdc3814c96556806709c4"
      },
      "cell_type": "code",
      "source": "rnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 0)\nrnd_clf.fit(train_df.iloc[:,1:29],train_df.iloc[:,30]);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "970d364dabbfbccbf888cda5068e707593261d18"
      },
      "cell_type": "code",
      "source": "x, y = (list(x) for x in zip(*sorted(zip(rnd_clf.feature_importances_, train_df.iloc[:,1:29].columns), \n                                                            reverse = False)))\ntrace2 = go.Bar(\n    x=x ,\n    y=y,\n    marker=dict(\n        color=x,\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n    name='Random Forest Feature importance',\n    orientation='h',\n)\n\nlayout = dict(\n    title='Barplot of Feature importances',\n     width = 600, height = 1000,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n#         domain=[0, 0.85],\n    ),\n    margin=dict(\n    l=300,\n),\n)\n\nfig1 = go.Figure(data=[trace2], layout=layout)\niplot(fig1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba7ee28c9e3c824b6dd454f3752bf155b7ca71d0"
      },
      "cell_type": "code",
      "source": "for name, importance in zip(train_df.iloc[:,1:29].columns, rnd_clf.feature_importances_):\n    if importance > 0.020 :\n        print('\"' + name + '\"'+',')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "20d84ce3-e99a-44ba-b39e-6f24aa489afe",
        "collapsed": true,
        "_uuid": "d9cda40f61bfcc22ce81b1bbe39ca951f31e0350",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df.drop(['V19','V21','V1','V2','V6','V5','V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fcf8bea9-3325-4e49-b552-8fa8022807e9",
        "_uuid": "abccdc97bfe4603c4eedc92ea2fdc3a6b9308d08"
      },
      "cell_type": "markdown",
      "source": "I have removed Amount and Time feature since they wont add much value in calculating gaussian distribution."
    },
    {
      "metadata": {
        "_cell_guid": "1e9c0a38-0536-4652-9cfa-3094a2bebf48",
        "collapsed": true,
        "_uuid": "8b434da9d51f825a467d60b99680f100e284fd6f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df.drop(labels = [\"Amount\",\"Time\"], axis = 1, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c23a9cd6-bc1d-4fd9-a786-d64ce4487f3a",
        "_uuid": "3c6668db06edcff208987fd08682be6a1c5802d8"
      },
      "cell_type": "markdown",
      "source": "Split the dataset into 2 part one with Class 1 and other with class 0"
    },
    {
      "metadata": {
        "_cell_guid": "ac4e594e-bb70-4496-b4d2-9357b80b99ad",
        "collapsed": true,
        "_uuid": "fe78e559d679120f6420a7b74d63ad9f92f0cbed",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_strip_v1 = train_df[train_df[\"Class\"] == 1]\ntrain_strip_v0 = train_df[train_df[\"Class\"] == 0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "93659d18-d7f2-4ecc-83ba-bd4443217984",
        "_uuid": "cb1c7496fec012a39e16dd97ef970a07308f055f"
      },
      "cell_type": "markdown",
      "source": "In the Anomalized technique  we distribute this large dataset into 3 parts .\n\n1) Normal Transactons: classified as 0 , no anomalized transaction should be present here since it is not a supervised method<br>  How to get this dataset :- 60% of normal transactions should be added here. <br> \nFind out Epsilon by using  min(Probability) command \n\n2) dataset for Cross validation : from the remaining normal transaction take 50 % (i.e. 20 % as a whole since we have already took the data in the first step)  and add 50% of the Anomalized data with this .\n\n3) dataset for testing the algorithm :- this step is similar to what we did for Cross validattion. <br>\n Test dataset = leftover normal transaction + leftover Anomalized data \n"
    },
    {
      "metadata": {
        "_cell_guid": "f6eef861-1ed0-4dc8-a98c-ce329b05b370",
        "_uuid": "ac713bf2ef332551825185dce57f84aa65e60dad",
        "trusted": true
      },
      "cell_type": "code",
      "source": "Normal_len = len (train_strip_v0)\nAnomolous_len = len (train_strip_v1)\n\nstart_mid = Anomolous_len // 2\nstart_midway = start_mid + 1\n\ntrain_cv_v1  = train_strip_v1 [: start_mid]\ntrain_test_v1 = train_strip_v1 [start_midway:Anomolous_len]\n\nstart_mid = (Normal_len * 60) // 100\nstart_midway = start_mid + 1\n\ncv_mid = (Normal_len * 80) // 100\ncv_midway = cv_mid + 1\n\ntrain_fraud = train_strip_v0 [:start_mid]\ntrain_cv    = train_strip_v0 [start_midway:cv_mid]\ntrain_test  = train_strip_v0 [cv_midway:Normal_len]\n\ntrain_cv = pd.concat([train_cv,train_cv_v1],axis=0)\ntrain_test = pd.concat([train_test,train_test_v1],axis=0)\n\n\nprint(train_fraud.columns.values)\nprint(train_cv.columns.values)\nprint(train_test.columns.values)\n\ntrain_cv_y = train_cv[\"Class\"]\ntrain_test_y = train_test[\"Class\"]\n\ntrain_cv.drop(labels = [\"Class\"], axis = 1, inplace = True)\ntrain_fraud.drop(labels = [\"Class\"], axis = 1, inplace = True)\ntrain_test.drop(labels = [\"Class\"], axis = 1, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bba5158d-47a5-4014-81c2-a137798e4cbf",
        "_uuid": "890be9349074ace71e60248ffd9ae5233415befe"
      },
      "cell_type": "markdown",
      "source": "Choosing Epsilon Values <br>\n    ---\nI calculated P value for all the rows present in Normal Transaction and found the minimum P value \nby using below command\n **min(p)** \n      ---\nsimilalrly I found the minimum P Value for rest of the datasets and found this value to be very close to 0 and then i found the max(p) value which is again somewhat far from 0. <br><br>\nInstead of looping between the epsilon values (between min and max of P) , i chose set of epsilon values for demonstration purpose to see how well i can perform to find the fraudulent transactions."
    },
    {
      "metadata": {
        "_cell_guid": "834c407e-47aa-40a4-9900-03ec7ef01ef2",
        "collapsed": true,
        "_uuid": "2dee73a4d60406fe5d833fc7573d4d43f8722060",
        "trusted": true
      },
      "cell_type": "code",
      "source": "mu, sigma = estimateGaussian(train_fraud)\np = multivariateGaussian(train_fraud,mu,sigma)\np_cv = multivariateGaussian(train_cv,mu,sigma)\np_test = multivariateGaussian(train_test,mu,sigma)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "09cb7cf7-8a57-42ad-bb5b-faddd6bd53c9",
        "_uuid": "1706a8a385b73bc8f8677e0d1ad51bb26599862a"
      },
      "cell_type": "markdown",
      "source": "Performance wrt to Epsilon values\n    ----\nCheck out how well we are performing with the given set of epsilon values from the function called here."
    },
    {
      "metadata": {
        "_cell_guid": "43f5e9b4-8125-46ca-bdae-70d3add290b8",
        "_uuid": "8320b88b4d055aff7b0d5db54e4d22928bfa6c38",
        "trusted": true
      },
      "cell_type": "code",
      "source": "fscore, ep= selectThresholdByCV(p_cv,train_cv_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d7593502-ef6a-4891-bf89-db49854332bd",
        "collapsed": true,
        "_uuid": "cd5399b33805a1b3fc07a9121aec1d2e709fed44"
      },
      "cell_type": "markdown",
      "source": "Epsilon value = 1.0527717316e-70 is selected as threshold to identify Anomalous transactions \n\nnow time to Predict and calculate  F1 , Recall and Precision score for our Test Dataset"
    },
    {
      "metadata": {
        "_cell_guid": "c60b8cca-6d84-47f8-91fd-08e7d161c2c9",
        "_uuid": "09d9efa434d6ebcfa9c4753840b55fdfdb102f3f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "predictions = (p_test < ep)\nRecall = recall_score(train_test_y, predictions, average = \"binary\")    \nPrecision = precision_score(train_test_y, predictions, average = \"binary\")\nF1score = f1_score(train_test_y, predictions, average = \"binary\")    \nprint ('F1 score , Recall and Precision for Test dataset')\nprint ('Best F1 Score %f' %F1score)\nprint ('Best Recall Score %f' %Recall)\nprint ('Best Precision Score %f' %Precision)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "32263e59-72a7-45e9-b3a1-1be0a8821dfe",
        "_uuid": "94a1153ef19da1e38a7efefbcf680fc1d0224633"
      },
      "cell_type": "markdown",
      "source": "Lets Visualize our predictions in below scatter plot \n         -------"
    },
    {
      "metadata": {
        "_cell_guid": "dbd3a061-c143-4e23-9992-d2be680f0509",
        "_uuid": "3c62c8c46eaf0b39c5f11b70c39ac689bfbcc3eb",
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(train_test['V14'],train_test['V11'],marker=\"o\", color=\"lightBlue\")\nax.set_title('Anomalies(in red) vs Predicted Anomalies(in Green)')\nfor i, txt in enumerate(train_test['V14'].index):\n       if train_test_y.loc[txt] == 1 :\n            ax.annotate('*', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=13,color='Red')\n       if predictions[i] == True :\n            ax.annotate('o', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=15,color='Green')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4b3f43b5-b021-4201-befa-2c6eaaff8fa1",
        "_uuid": "a4abe1dc06fa5b5d6e3bb8f08704671fe2205dc9"
      },
      "cell_type": "markdown",
      "source": "From the above result we can see that we are able to maintain the balance between Recall and Precision. \n\nPrecision of around 87% with Recall of 68% is not bad at all when we have such highly unbalanced data. \nThese numbers are not fixed and can vary . \n \n These numbers were different for Cross validation dataset and we shortlisted our Epsilon value by comparing the results of F1 Score.\n\nI will show you the result we achieved on Cross validation dataset again."
    },
    {
      "metadata": {
        "_cell_guid": "00b86679-7e69-461e-a93b-a9cb82528912",
        "_uuid": "c0502af04356d265281f2d736e4c5f9d3b3d6744",
        "trusted": true
      },
      "cell_type": "code",
      "source": "predictions = (p_cv < ep)\nRecall = recall_score(train_cv_y, predictions, average = \"binary\")    \nPrecision = precision_score(train_cv_y, predictions, average = \"binary\")\nF1score = f1_score(train_cv_y, predictions, average = \"binary\")    \nprint ('F1 score , Recall and Precision for Cross Validation dataset')\nprint ('Best F1 Score %f' %F1score)\nprint ('Best Recall Score %f' %Recall)\nprint ('Best Precision Score %f' %Precision)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6898077a-922a-44cc-9352-f26224793676",
        "_uuid": "6ee6a6495c9c5c891f6e250aa2e323e80551bf65"
      },
      "cell_type": "markdown",
      "source": "\n Summary of above Algorithm: \n \n 1) Find Epsilon value by considering only Normal Transaction.\n \n 2) Use this Epsilon value on CV dataset (Normal transaction + Anomalous transaction)\n \n 3) Come up with set of Epsilon values to see how your algorithm performs and note down the Best F1 score along with\n      Recall and Precision percentage \n      \n 4) Choose the Epsilon value with highest F1 score \n \n 5) Use this Epsilon value to predict the Anomalous transaction on Test Dataset   \n \nPlease comment and let me know to help improve this kernel."
    },
    {
      "metadata": {
        "_cell_guid": "07750393-1be0-4c53-9a84-a9fd06f9f497",
        "_uuid": "7cb720ed036a0c96a17fac0d73c7e6b2c0088e5f"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_cell_guid": "20c81f4a-463c-4698-b9b2-d206a42d71aa",
        "_uuid": "c9059245eaee9da475209aed05aa3d2eb20f0e6f"
      },
      "cell_type": "markdown",
      "source": ""
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}